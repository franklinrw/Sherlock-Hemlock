{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers as t\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 570/570 [00:00<00:00, 282kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to C:/Users/Frank/.cache/huggingface/datasets/FranklinWillemen___parquet/FranklinWillemen--mapa_plus-4bbc20eeb61798b9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 326k/326k [00:00<00:00, 894kB/s]\n",
      "Downloading data: 100%|██████████| 700k/700k [00:00<00:00, 1.53MB/s]]\n",
      "Downloading data: 100%|██████████| 1.98M/1.98M [00:00<00:00, 2.59MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 223.16it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/Frank/.cache/huggingface/datasets/FranklinWillemen___parquet/FranklinWillemen--mapa_plus-4bbc20eeb61798b9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 111.45it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"FranklinWillemen/mapa_plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['language', 'tokens', 'coarse_grained', 'fine_grained', 'pos', 'dep', 'heads'],\n",
       "        num_rows: 1040\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['language', 'tokens', 'coarse_grained', 'fine_grained', 'pos', 'dep', 'heads'],\n",
       "        num_rows: 3081\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['language', 'tokens', 'coarse_grained', 'fine_grained', 'pos', 'dep', 'heads'],\n",
       "        num_rows: 8341\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dataset[\"train\"])\n",
    "df_val = pd.DataFrame(dataset[\"validation\"])\n",
    "df_test = pd.DataFrame(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode_list(series):\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Concatenate all the lists in the series to fit the encoder\n",
    "    concatenated = [item for sublist in series for item in sublist]\n",
    "    label_encoder.fit(concatenated)\n",
    "    \n",
    "    # Transform each list separately and store in a new series\n",
    "    encoded_series = series.apply(lambda x: label_encoder.transform(x))\n",
    "    \n",
    "    return encoded_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_instance(tokens, pos_encoded, heads, labels_encoded):\n",
    "    num_nodes = len(tokens)\n",
    "    \n",
    "    # Create nodes and assign features\n",
    "    x = torch.tensor(pos_encoded, dtype=torch.long).view(-1, 1)\n",
    "    \n",
    "    # Convert heads from string to int and create edges using head indices\n",
    "    heads = list(map(int, heads))\n",
    "    edge_index = [[head_idx, idx] for idx, head_idx in enumerate(heads)]\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Assign NER labels as ground truth\n",
    "    y = torch.tensor(labels_encoded, dtype=torch.long)\n",
    "    \n",
    "    # Create Data instance for PyTorch Geometric\n",
    "    graph = Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoded_train = integer_encode_list(df_train['pos'])\n",
    "labels_encoded_train = integer_encode_list(df_train['coarse_grained'])\n",
    "\n",
    "pos_encoded_val = integer_encode_list(df_val['pos'])\n",
    "labels_encoded_val = integer_encode_list(df_val['coarse_grained'])\n",
    "\n",
    "pos_encoded_test = integer_encode_list(df_test['pos'])\n",
    "labels_encoded_test = integer_encode_list(df_test['coarse_grained'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_train = [create_graph_instance(tokens, pos, heads, labels)\n",
    "          for tokens, pos, heads, labels in zip(df_train['tokens'], pos_encoded_train, df_train['heads'], labels_encoded_train)]\n",
    "\n",
    "graphs_val = [create_graph_instance(tokens, pos, heads, labels)\n",
    "          for tokens, pos, heads, labels in zip(df_val['tokens'], pos_encoded_val, df_val['heads'], labels_encoded_val)]\n",
    "\n",
    "graphs_test = [create_graph_instance(tokens, pos, heads, labels)\n",
    "          for tokens, pos, heads, labels in zip(df_test['tokens'], pos_encoded_test, df_test['heads'], labels_encoded_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
