{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list_of_lists(df, col_name):\n",
    "    df[col_name] = df[col_name].apply(lambda x: ast.literal_eval(x) if not isinstance(x, list) else x)\n",
    "    return df\n",
    "\n",
    "def integer_encode_list(series):\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Concatenate all the lists in the series to fit the encoder\n",
    "    concatenated = [item for sublist in series for item in sublist if isinstance(item, str)]\n",
    "    unique_elements = list(set(concatenated))\n",
    "    label_encoder.fit(unique_elements)\n",
    "    \n",
    "    # Transform each list separately and store in a new series\n",
    "    encoded_series = series.apply(lambda x: [label_encoder.transform([elem]).tolist()[0] if isinstance(elem, str) else elem for elem in x])\n",
    "    \n",
    "    return encoded_series\n",
    "\n",
    "def create_graph_instance(tokens, pos_encoded, heads, word_embeddings, article_id):\n",
    "    sentence_graphs = []\n",
    "    \n",
    "    for sentence_tokens, sentence_pos, sentence_heads, sentence_word_embeddings in zip(tokens, pos_encoded, heads, word_embeddings):\n",
    "        # Create a dictionary to map tokens to their corresponding embeddings\n",
    "        token_embedding_map = {embedding_tuple[1]: embedding_tuple[2] for embedding_tuple in sentence_word_embeddings}\n",
    "        \n",
    "        # Create a list of tuples containing the pos tag and the corresponding word embedding, or zeros if the token has no embedding\n",
    "        node_features = [(pos, token_embedding_map.get(token, np.zeros(300))) for pos, token in zip(sentence_pos, sentence_tokens)]\n",
    "        \n",
    "        # Create nodes and assign features\n",
    "        node_features = torch.tensor([([pos] if type(pos) == int else [pos]) + list(embedding) for pos, embedding in node_features], dtype=torch.float).view(-1, 301)\n",
    "        \n",
    "        # Convert heads from string to int and create edges using head indices\n",
    "        sentence_heads = list(map(int, sentence_heads))\n",
    "        edge_index = [[head_idx, idx] for idx, head_idx in enumerate(sentence_heads)]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Assign article_id as ground truth\n",
    "        y = torch.tensor(article_id, dtype=torch.long)\n",
    "        \n",
    "        # Create Data instance for PyTorch Geometric\n",
    "        graph = Data(x=node_features, edge_index=edge_index, y=y)\n",
    "        sentence_graphs.append(graph)\n",
    "    \n",
    "    return sentence_graphs\n",
    "\n",
    "def process_dataframe(hdf5_path):\n",
    "    # Load the dataframe from the hdf5 file\n",
    "    df = pd.read_hdf(hdf5_path)\n",
    "\n",
    "    # Convert string representations of lists to actual lists\n",
    "    columns_to_convert = ['pos', 'dep', 'heads', 'tokens', 'word_embeddings']\n",
    "    for col in columns_to_convert:\n",
    "        df = string_to_list_of_lists(df, col)\n",
    "\n",
    "    # Integer encode the pos and dep columns\n",
    "    pos_encoded = integer_encode_list(df['pos'])\n",
    "    dep_encoded = integer_encode_list(df['dep'])\n",
    "\n",
    "    # Create graphs\n",
    "    graphs = []\n",
    "    for i, row in df.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        pos = pos_encoded.loc[i]\n",
    "        dep = dep_encoded.loc[i]\n",
    "        heads = row['heads']\n",
    "        word_embeddings = row['word_embeddings']\n",
    "\n",
    "        # Check if the current dataframe is a question dataframe\n",
    "        if 'article_ids' in df.columns:\n",
    "            article_ids = ast.literal_eval(row['article_ids'])\n",
    "            if isinstance(article_ids, tuple):\n",
    "                for article_id in article_ids:\n",
    "                    graphs.extend(create_graph_instance(tokens, pos, heads, word_embeddings, article_id))\n",
    "            else:\n",
    "                graphs.extend(create_graph_instance(tokens, pos, heads, word_embeddings, article_ids))\n",
    "        else:\n",
    "            article_id = row['id']\n",
    "            graphs.extend(create_graph_instance(tokens, pos, heads, word_embeddings, article_id))\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q_test_graphs \u001b[39m=\u001b[39m process_dataframe(\u001b[39m\"\u001b[39;49m\u001b[39m../../local_datasets/bsard_extra/gsard_expert_questions_test.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[38], line 75\u001b[0m, in \u001b[0;36mprocess_dataframe\u001b[1;34m(hdf5_path)\u001b[0m\n\u001b[0;32m     73\u001b[0m             graphs\u001b[39m.\u001b[39mextend(create_graph_instance(tokens, pos, heads, word_embeddings, article_id))\n\u001b[0;32m     74\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m         graphs\u001b[39m.\u001b[39mextend(create_graph_instance(tokens, pos, heads, word_embeddings, article_ids))\n\u001b[0;32m     76\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     article_id \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[38], line 29\u001b[0m, in \u001b[0;36mcreate_graph_instance\u001b[1;34m(tokens, pos_encoded, heads, word_embeddings, article_id)\u001b[0m\n\u001b[0;32m     26\u001b[0m node_features \u001b[39m=\u001b[39m [(pos, token_embedding_map\u001b[39m.\u001b[39mget(token, np\u001b[39m.\u001b[39mzeros(\u001b[39m300\u001b[39m))) \u001b[39mfor\u001b[39;00m pos, token \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sentence_pos, sentence_tokens)]\n\u001b[0;32m     28\u001b[0m \u001b[39m# Create nodes and assign features\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m node_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([([pos] \u001b[39mif\u001b[39;49;00m \u001b[39mtype\u001b[39;49m(pos) \u001b[39m==\u001b[39;49m \u001b[39mint\u001b[39;49m \u001b[39melse\u001b[39;49;00m [pos]) \u001b[39m+\u001b[39;49m \u001b[39mlist\u001b[39;49m(embedding) \u001b[39mfor\u001b[39;49;00m pos, embedding \u001b[39min\u001b[39;49;00m node_features], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m301\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Convert heads from string to int and create edges using head indices\u001b[39;00m\n\u001b[0;32m     32\u001b[0m sentence_heads \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, sentence_heads))\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "q_test_graphs = process_dataframe(\"../../local_datasets/bsard_extra/gsard_expert_questions_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"../../local_datasets/bsard_extra/gsard_expert_questions_test.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list_of_lists(df, col_name):\n",
    "    df[col_name] = df[col_name].apply(lambda x: ast.literal_eval(x) if not isinstance(x, list) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'category', 'subcategory', 'question', 'extra_description',\n",
       "       'article_ids', 'normalized_question', 'pos', 'dep', 'heads', 'tokens',\n",
       "       'word_embeddings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = string_to_list_of_lists(df, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['quels', 'sont', 'les', 'critères', 'communaux', 'd', \"'\", 'insalubrité']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode_list(series):\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Concatenate all the lists in the series to fit the encoder\n",
    "    concatenated = [item for sublist in series for item in sublist if isinstance(item, str)]\n",
    "    unique_elements = list(set(concatenated))\n",
    "    label_encoder.fit(unique_elements)\n",
    "    \n",
    "    # Transform each list separately and store in a new series\n",
    "    encoded_series = series.apply(lambda x: [label_encoder.transform([elem]).tolist()[0] if isinstance(elem, str) else elem for elem in x])\n",
    "    \n",
    "    return encoded_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ADJ', 'AUX', 'DET', 'NOUN', 'ADJ', 'ADP', 'ADP', 'NOUN']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pos\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [[ADJ, AUX, DET, NOUN, ADJ, ADP, ADP, NOUN]]\n",
       "1      [[AUX, PRON, PRON, NOUN, ADP, DET, DET, NOUN, ...\n",
       "2      [[DET, NOUN, VERB, PRON, VERB, ADV, ADP, ADP, ...\n",
       "3             [[ADV, PRON, VERB, DET, NOUN, ADP, PROPN]]\n",
       "4      [[VERB, PRON, PRON, PRON, VERB, ADP, NOUN, ADP...\n",
       "                             ...                        \n",
       "217    [[PRON, VERB, NOUN, ADJ, PUNCT, VERB, PRON, VE...\n",
       "218           [[ADP, PRON, VERB, PRON, VERB, DET, NOUN]]\n",
       "219    [[PRON, AUX, ADJ, VERB, VERB, PUNCT, PRON, VER...\n",
       "220    [[AUX, PRON, SCONJ, PRON, VERB, VERB, DET, NOU...\n",
       "221    [[PRON, AUX, NOUN, ADP, NOUN, ADJ, PUNCT, ADP,...\n",
       "Name: pos, Length: 222, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encode_list(df[\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
